# -*- coding: utf-8 -*-
"""CNNProject-WeatherClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kZXjCdU-xvrXRae2zgOqJBdjlA-ChiuS
"""

# Commented out IPython magic to ensure Python compatibility.
import streamlit as st
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns
from tqdm import tqdm

# %matplotlib inline
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms, models
from torchvision.utils import make_grid
from torchsummary import summary
from collections import Counter

import kagglehub
import os
from PIL import Image

# Download latest version
# path = kagglehub.dataset_download("jehanbhathena/weather-dataset")
# print("Path to dataset files:", path)

# os.system("kaggle datasets download jehanbhathena/weather-dataset --unzip -p ./dataset")

image_size = 224
batch_size = 32
num_workers = 2
learning_rate = 1e-4
num_epoch = 100

tf_train = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.25),
    transforms.RandomVerticalFlip(p=0.25),
    transforms.RandomRotation(10),
    transforms.Resize((image_size, image_size)),
    transforms.ToTensor()
    ])
tf_test = transforms.Compose([
    transforms.Resize((image_size, image_size)),
    transforms.ToTensor()

    ])

dataset_path = os.path.abspath('weatherimages/dataset/')  # Modify this based on your folder structure
train_path = os.path.abspath('weatherimages/train/')
test_path = os.path.abspath('weatherimages/test/')
# Check if dataset exists
if not os.path.exists(dataset_path):
    st.error(f"Dataset path {dataset_path} does not exist!")
else:
    st.write(f"Dataset found at {dataset_path}")

# Transformations
tf_train = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.25),
    transforms.RandomVerticalFlip(p=0.25),
    transforms.RandomRotation(10),
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

tf_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

# Load dataset
try:
    dataset = datasets.ImageFolder(dataset_path, transform=None)
    train_set, test_set = random_split(dataset, lengths=[int(0.8 * len(dataset)), int(0.2 * len(dataset))])
except Exception as e:
    st.error(f"Error loading dataset: {e}")

# # Create directories for train and test splits
# os.makedirs(train_path, exist_ok=True)
# os.makedirs(test_path, exist_ok=True)

def save_split_dataset(dataset, folder):
    """ Save split dataset into respective folders for training and testing """
    for i, (image, label) in enumerate(dataset):
        class_dir = os.path.join(folder, dataset.dataset.classes[label])
        os.makedirs(class_dir, exist_ok=True)  # Create class subdirectory if it doesn't exist
        image_path = os.path.join(class_dir, f'image_{i}.jpg')
        image.save(image_path)  # Save the image to the directory

# Save the split datasets
save_split_dataset(train_set, train_path)
save_split_dataset(test_set, test_path)

# Step 6: Load the split datasets with transforms
train_set = datasets.ImageFolder(train_path, transform=tf_train)
test_set = datasets.ImageFolder(test_path, transform=tf_test)

# Create DataLoaders for both datasets
train_loader = DataLoader(train_set, batch_size=32, shuffle=True)
test_loader = DataLoader(test_set, batch_size=32, shuffle=False)

# Function to count samples per class
# def count_samples_per_class(dataset):
#     labels = [label for _, label in dataset]
#     label_count = Counter(labels)
#     return label_count

# label_count = count_samples_per_class(train_set)
# num_classes = len(label_count)
# print("Total number of data: ", len(train_set))
# print("Number of classes:", num_classes)
# print("Number of data samples per class:", label_count)

# label_count = count_samples_per_class(test_set)
# num_classes = len(label_count)
# print("Total number of data: ", len(test_set))
# print("Number of classes:", num_classes)
# print("Number of data samples per class:", label_count)

cat_to_name = {'1': 'dew', '2': 'fogsmog', '3': 'frost', '4':'glaze', '5':'hail', '6':'lightning', '7':'rain', '8':'rainbow', '9':'rime', '10':'sandstorm', '11':'snow'}

def imshow(img_tensor, ax, label):
    """ Utility function to display an image tensor with a caption. """
    img_tensor = img_tensor.clamp(0, 1)  # Ensure values are in the range [0, 1] for floating point
    if img_tensor.shape[0] == 3:  # Check if it's an RGB image
        ax.imshow(img_tensor.permute(1, 2, 0))  # Reorder dimensions to (height, width, channels)
    else:
        ax.imshow(img_tensor[0], cmap='gray')  # Display the first channel (grayscale)
    ax.axis('off')
    ax.set_title(label, fontsize=10)

def show_images_batch(images_batch, labels):
    """
    Display a batch of images in a grid with labels.
    """

    batch_size = images_batch.shape[0]
    grid_cols = int(np.ceil(np.sqrt(batch_size)))
    grid_rows = int(np.ceil(batch_size / grid_cols))  # Ensure all images fit in the grid

    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(grid_cols * 1.5, grid_rows * 1.5))
    axes = np.atleast_1d(axes)  # Ensure axes are always iterable

    for i in range(grid_rows * grid_cols):
        ax = axes.flatten()[i]
        if i < batch_size:
            imshow(images_batch[i], ax, cat_to_name[str(labels[i].item() + 1)])
        else:
            ax.axis('off')

    plt.tight_layout()
    plt.show()

for idx, data in enumerate(train_loader):

  show_images_batch(images_batch=data[0], labels=data[1])
  break

for idx, data in enumerate(test_loader):

  show_images_batch(images_batch=data[0], labels=data[1])
  break

"""### CNN Model"""

convnext_tiny = models.convnext_tiny(weights='IMAGENET1K_V1').cuda()

summary(convnext_tiny, (3, 224, 224))

# close all the parameter to be trained
for p in convnext_tiny.parameters():
  p.requires_grad = False

for n,p in convnext_tiny.named_parameters():
  # print(n)
  if 'features.1' in n or 'features.4' in n :
    p.requires_grad = True

# convnext_tiny.features[-1].shape()

summary(convnext_tiny, (3, 224, 224))

num_class = 11

class CNNModel(nn.Module):

  def __init__(self, image_depth):

    super().__init__()
    self.convnext_tiny = convnext_tiny

    self.fc1 = nn.Linear(in_features=1000,out_features=512)
    self.fc2 = nn.Linear(in_features=512,out_features=128)
    self.fc3 = nn.Linear(in_features=128,out_features=32)
    self.out = nn.Linear(in_features=32,out_features=num_class)

  def forward(self, x):
    x = self.convnext_tiny(x)

    #fully-connected classifiers.
    x = torch.flatten(x, start_dim=1) #we need to all the dimensions except the batch. 0 is batch
    x = F.relu(self.fc1(x))
    x = F.dropout(x,p=0.45)
    x = F.relu(self.fc2(x))
    x = F.dropout(x,p=0.25)
    x = F.relu(self.fc3(x))
    x = F.dropout(x,p=0.1)
    x = self.out(x)

    return x

model = CNNModel(3)
model = model.cuda()

summary(model, (3, 224, 224))

optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
loss_fn = nn.CrossEntropyLoss()

best_val_accuracy = 0
train_loss_history = []
train_acc_history = []
val_loss_history = []
val_acc_history = []
all_preds = []
all_labels = []
# for epoch_idx in range(num_epoch):
for epoch_idx in range(15):

  train_running_loss = 0.0
  train_running_accuracy = 0.0

  idx = 0
  for idx, data in tqdm(enumerate(train_loader)):

    optimizer.zero_grad()

    images, labels = data
    images = images.cuda()
    labels = labels.cuda()

    pred = model(images)

    loss = loss_fn(pred, labels)

    loss.backward()
    optimizer.step()

    train_running_loss += loss.item()

    _, pred_arr = torch.max(pred.detach(), 1)
    label_arr = labels.cpu().numpy()


    train_running_accuracy += accuracy_score(label_arr, pred_arr.cpu().numpy())

  # Store the average training loss and accuracy for the current epoch
  train_loss_history.append(train_running_loss)
  train_acc_history.append(train_running_accuracy/(idx+1))

  print(f"Training Loss at epoch {epoch_idx} is {train_running_loss}")
  print(f"Training Accuracy at epoch {epoch_idx} is {train_running_accuracy/(idx+1)}")

for epoch_idx in range(15):
  val_running_loss = 0.0
  val_running_accuracy = 0.0
  idx = 0
  for idx, data in tqdm(enumerate(test_loader)):

    images, labels = data
    images = images.cuda()
    labels = labels.cuda()

    pred = model(images)
    loss = loss_fn(pred, labels)

    val_running_loss += loss.item()

    _, pred_arr = torch.max(pred.detach(), 1)
    label_arr = labels.cpu().numpy()
    all_preds.append(pred_arr.cpu().numpy())  # Move to CPU and convert to numpy
    all_labels.append(labels.cpu().numpy())   # Move to CPU and convert to numpy

    val_running_accuracy += accuracy_score(label_arr, pred_arr.cpu().numpy())

  # Store the average validation loss and accuracy for the current epoch
  val_loss_history.append(val_running_loss)
  val_acc_history.append(val_running_accuracy/(idx+1))

  print(f"Validation Loss at epoch {epoch_idx} is {val_running_loss}")
  print(f"Validation Accuracy at epoch {epoch_idx} is {val_running_accuracy/(idx+1)}")

"""### Performance"""

# Calculate average accuracy
val_running_accuracy /= (idx + 1)

# Concatenate all predictions and labels for confusion matrix
all_preds = np.concatenate(all_preds)
all_labels = np.concatenate(all_labels)

# Compute the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_preds)

# Plot the confusion matrix using seaborn for better visualization
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=range(num_class), yticklabels=range(num_class))
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

plt.figure(figsize=(12, 6))
num_epochs = len(train_loss_history)

# Loss plot
plt.subplot(1, 2, 1)
plt.plot(train_loss_history, label='Training Loss', color='blue')
plt.plot(val_loss_history, label='Validation Loss', color='red')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()

# Accuracy plot
plt.subplot(1, 2, 2)
plt.plot(train_acc_history, label='Training Accuracy', color='blue')
plt.plot(val_acc_history, label='Validation Accuracy', color='red')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Get precision, recall, f1-score, and support using classification_report
report = classification_report(all_labels, all_preds, output_dict=True)

# Extract overall precision, recall, f1-score from the 'macro avg' or 'weighted avg'
overall_precision = report['macro avg']['precision']
overall_recall = report['macro avg']['recall']
overall_f1_score = report['macro avg']['f1-score']
accuracy = report['accuracy']

# Print the overall metrics
print(f"Overall Precision: {overall_precision:.4f}")
print(f"Overall Recall: {overall_recall:.4f}")
print(f"Overall F1-Score: {overall_f1_score:.4f}")
print(f"Overall Accuracy: {accuracy:.4f}")

# Visualize the overall metrics as a bar plot
overall_metrics = {
    'Precision': overall_precision,
    'Recall': overall_recall,
    'F1-Score': overall_f1_score,
    'Accuracy': accuracy
}

plt.figure(figsize=(8, 10))
plt.bar(overall_metrics.keys(), overall_metrics.values(), color='lightseagreen')
plt.ylim(0.5, 1)
plt.title('Overall Precision, Recall, F1-Score, and Accuracy')
plt.ylabel('Score')
plt.show()

torch.save(model, 'convnextCNN_model.pth')
# torch.save(model.state_dict(), 'convnextCNN_model.pth')